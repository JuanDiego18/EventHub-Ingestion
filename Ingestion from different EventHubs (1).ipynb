{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fffeeca-751e-4d6f-b7df-9b9314ff54f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Event Hubs configuration and authentication\n",
    "EH_NAMESPACE                    = spark.conf.get(\"iot.ingestion.eh.namespace\")\n",
    "EH_NAME_1                         = spark.conf.get(\"iot.ingestion.eh.name\")\n",
    "EH_NAME_2                         = spark.conf.get(\"iot.ingestion.eh.name2\")\n",
    "\n",
    "EH_CONN_SHARED_ACCESS_KEY_NAME  = spark.conf.get(\"iot.ingestion.eh.accessKeyName\")\n",
    "SECRET_SCOPE                    = spark.conf.get(\"io.ingestion.eh.secretsScopeName\")\n",
    "EH_CONN_SHARED_ACCESS_KEY_VALUE = dbutils.secrets.get(scope = SECRET_SCOPE, key = EH_CONN_SHARED_ACCESS_KEY_NAME)\n",
    "\n",
    "EH_CONN_STR                     = f\"Endpoint=sb://{EH_NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={EH_CONN_SHARED_ACCESS_KEY_NAME};SharedAccessKey={EH_CONN_SHARED_ACCESS_KEY_VALUE}\"\n",
    "\n",
    "\n",
    "# Kafka Consumer configuration\n",
    "\n",
    "KAFKA_OPTIONS_TAXI = {\n",
    "  \"kafka.bootstrap.servers\"  : f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "  \"subscribe\"                : EH_NAME_1,\n",
    "  \"kafka.sasl.mechanism\"     : \"PLAIN\",\n",
    "  \"kafka.security.protocol\"  : \"SASL_SSL\",\n",
    "  \"kafka.sasl.jaas.config\"   : f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{EH_CONN_STR}\\\";\",\n",
    "  \"kafka.request.timeout.ms\" : spark.conf.get(\"iot.ingestion.kafka.requestTimeout\"),\n",
    "  \"kafka.session.timeout.ms\" : spark.conf.get(\"iot.ingestion.kafka.sessionTimeout\"),\n",
    "  \"maxOffsetsPerTrigger\"     : spark.conf.get(\"iot.ingestion.spark.maxOffsetsPerTrigger\"),\n",
    "  \"failOnDataLoss\"           : spark.conf.get(\"iot.ingestion.spark.failOnDataLoss\"),\n",
    "  \"startingOffsets\"          : spark.conf.get(\"iot.ingestion.spark.startingOffsets\")\n",
    "}\n",
    "\n",
    "KAFKA_OPTIONS_WEATHER = {\n",
    "  \"kafka.bootstrap.servers\"  : f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "  \"subscribe\"                : EH_NAME_2,\n",
    "  \"kafka.sasl.mechanism\"     : \"PLAIN\",\n",
    "  \"kafka.security.protocol\"  : \"SASL_SSL\",\n",
    "  \"kafka.sasl.jaas.config\"   : f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{EH_CONN_STR}\\\";\",\n",
    "  \"kafka.request.timeout.ms\" : spark.conf.get(\"iot.ingestion.kafka.requestTimeout\"),\n",
    "  \"kafka.session.timeout.ms\" : spark.conf.get(\"iot.ingestion.kafka.sessionTimeout\"),\n",
    "  \"maxOffsetsPerTrigger\"     : spark.conf.get(\"iot.ingestion.spark.maxOffsetsPerTrigger\"),\n",
    "  \"failOnDataLoss\"           : spark.conf.get(\"iot.ingestion.spark.failOnDataLoss\"),\n",
    "  \"startingOffsets\"          : spark.conf.get(\"iot.ingestion.spark.startingOffsets\")\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Basic record parsing and adding ETL audit columns\n",
    "def parse_taxi(df):\n",
    "  return (df\n",
    "    .withColumn(\"records\", col(\"value\").cast(\"string\"))\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"records\"), None, {\"schemaLocationKey\": \"taxi_schema\"})\n",
    ")  \n",
    "  )\n",
    "\n",
    "def parse_weather(df):\n",
    "  return (df\n",
    "    .withColumn(\"records\", col(\"value\").cast(\"string\"))\n",
    "    .withColumn(\"parsed_data\", from_json(col(\"records\"), None, {\"schemaLocationKey\": \"weather_schema\"})\n",
    ")    \n",
    "  )\n",
    "\n",
    "@dlt.create_table(\n",
    "  comment=\"Raw Taxi Events\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\",\n",
    "    \"pipelines.reset.allowed\": \"false\" # preserves the data in the delta table if you do full refresh\n",
    "  },\n",
    ")\n",
    "@dlt.expect(\"valid_topic\", \"topic IS NOT NULL\")\n",
    "@dlt.expect(\"valid records\", \"parsed_data IS NOT NULL\")\n",
    "def taxi_raw():\n",
    "  return (\n",
    "   spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS_TAXI)\n",
    "    .load()\n",
    "    .transform(parse_taxi)\n",
    "  )\n",
    "\n",
    "@dlt.create_table(\n",
    "    comment=\"Raw Weather Events\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "@dlt.expect(\"valid_topic\", \"topic IS NOT NULL\")\n",
    "@dlt.expect(\"valid records\", \"parsed_data IS NOT NULL\")\n",
    "def weather_raw():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .options(**KAFKA_OPTIONS_WEATHER)\n",
    "        .load()\n",
    "        .transform(parse_weather)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d89d7d-ed92-4874-88ed-2bb02af3363e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver table with flattened taxi_parsed struct\"\n",
    ")\n",
    "def silver_trip_data():\n",
    "    df = dlt.readStream(\"taxi_raw\")\n",
    "    return df.select(\n",
    "        col(\"parsed_data._rescued_data\").alias(\"rescued_data\"),\n",
    "        col(\"parsed_data.doLocationId\").alias(\"doLocationId\"),\n",
    "        col(\"parsed_data.extra\").alias(\"extra\"),\n",
    "        col(\"parsed_data.fareAmount\").alias(\"fareAmount\"),\n",
    "        col(\"parsed_data.improvementSurcharge\").alias(\"improvementSurcharge\"),\n",
    "        col(\"parsed_data.mtaTax\").alias(\"mtaTax\"),\n",
    "        col(\"parsed_data.passengerCount\").alias(\"passengerCount\"),\n",
    "        col(\"parsed_data.paymentType\").alias(\"paymentType\"),\n",
    "        col(\"parsed_data.puLocationId\").alias(\"puLocationId\"),\n",
    "        col(\"parsed_data.rateCodeId\").alias(\"rateCodeId\"),\n",
    "        col(\"parsed_data.storeAndFwdFlag\").alias(\"storeAndFwdFlag\"),\n",
    "        col(\"parsed_data.tipAmount\").alias(\"tipAmount\"),\n",
    "        col(\"parsed_data.tollsAmount\").alias(\"tollsAmount\"),\n",
    "        col(\"parsed_data.totalAmount\").alias(\"totalAmount\"),\n",
    "        col(\"parsed_data.tpepDropoffDateTime\").alias(\"tpepDropoffDateTime\"),\n",
    "        col(\"parsed_data.tpepPickupDateTime\").alias(\"tpepPickupDateTime\"),\n",
    "        col(\"parsed_data.tripDistance\").alias(\"tripDistance\"),\n",
    "        col(\"parsed_data.vendorID\").alias(\"vendorID\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion from different EventHubs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}